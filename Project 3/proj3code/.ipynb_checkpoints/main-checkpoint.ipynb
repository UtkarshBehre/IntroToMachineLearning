{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrzF_4RKfPpj"
   },
   "source": [
    "# CSE574 Intro to Machine Learning Project 3\n",
    "### Train unsupervised models: kmeans, auto-encoder based kmeans and gmm using Fashion-MNIST clothing images \n",
    "### author: Utkarsh Behre (ubehre@buffalo.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtBD_3l1ymXQ"
   },
   "source": [
    "### Task 1: Use KMeans algorithm to cluster original data space of Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olyD69gfymXU"
   },
   "source": [
    "#### Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hyof08WzfPpt",
    "outputId": "3d1141f9-4d69-4b0d-bec5-c2b4ba448031",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Read Fashion MNIST dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "# Below code can be used to get data from local files by uncommenting section 2\n",
    "# Section 1\n",
    "# import util_mnist_reader as mnist_reader\n",
    "# X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "# X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "# Below code can be used to get data from tensorflow datasets directly when running in google collab\n",
    "# Section 2\n",
    "import tensorflow as tf\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4wdtzWLymXr"
   },
   "source": [
    "#### preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "m64zJy66ymXu",
    "outputId": "90664add-6644-4a52-9330-4273efc9ee8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max values of train and test set before normalizing:  0 255 0 255\n",
      "Min and Max values of train and test set after normalizing:  0.0 1.0 0.0 1.0\n",
      "X_train shape:  (60000, 784) , y_train shape:  (60000,) , X_test shape:  (10000, 784) , y_test shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# First we reshape the 28x28 images to 784 pixel values\n",
    "X_train = X_train.reshape(60000,784)\n",
    "X_test = X_test.reshape(10000,784)\n",
    "\n",
    "# then we normalize the values\n",
    "print(\"Min and Max values of train and test set before normalizing: \", np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test))\n",
    "X_train = X_train/np.max(X_train)\n",
    "X_test = X_test/np.max(X_test)\n",
    "print(\"Min and Max values of train and test set after normalizing: \", np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test))\n",
    "\n",
    "# print final shapes of training set and testing set\n",
    "print(\"X_train shape: \", X_train.shape, \", y_train shape: \", y_train.shape,\", X_test shape: \", X_test.shape, \", y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGW23pWsymX9"
   },
   "source": [
    "#### make the kmeans model using sklearn library and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sHtLDNfhfPp9",
    "outputId": "e05a9d4d-d450-445f-a4ed-4397fcdc9561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=100,\n",
       "       n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tol is the relative tolerance required to make sure it doesnt take too much time to find convergence\n",
    "kmeans = KMeans(n_clusters=10, init='random', max_iter=100, tol=0.0001)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii7tqoO5fPqG"
   },
   "source": [
    "#### test the model on test dataset and get accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "oChbj4TPfPqP",
    "outputId": "a138eb1d-d3ff-40da-e8c7-2381561ad823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5   0   4   3   5   0   0   0 408   2]\n",
      " [ 29 890   4 503  27   0  12   0   6   0]\n",
      " [587  50  19 277 136   0 189   0   3   0]\n",
      " [ 34   9 566  10 627   0 312   0  62   0]\n",
      " [ 93  22  61  92  42 649 115  62  84  29]\n",
      " [245  29 342 113 159   6 357   0  36   4]\n",
      " [  0   0   0   0   0  44   0   2   1 407]\n",
      " [  6   0   4   2   4   0  15   0 351   0]\n",
      " [  1   0   0   0   0 229   0 791  42  26]\n",
      " [  0   0   0   0   0  72   0 145   7 532]]\n",
      "Task1: Accuracy on the Training dataset 52.37642972025175\n",
      "Task1: Accuracy on the Testing dataset 52.4709193770049\n"
     ]
    }
   ],
   "source": [
    "y_predict_train = kmeans.labels_\n",
    "y_predict_test = kmeans.predict(X_test)\n",
    "\n",
    "score1 = 100 * completeness_score(y_train, y_predict_train)\n",
    "score2 = 100 * completeness_score(y_test, y_predict_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_predict_test, y_test)\n",
    "print(conf_matrix)\n",
    "print(\"Task1: Accuracy on the Training dataset\", score1)\n",
    "print(\"Task1: Accuracy on the Testing dataset\", score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvLAsjaxfPqY"
   },
   "source": [
    "### Task 2 and Task 3's Auto-Encoder part: Build an Auto-Encoder to have the condensed representation of the unlabeled fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axZDgFdUymYw"
   },
   "source": [
    "#### Importing required libraries for task 2 and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "li0qH1ZUfPqb",
    "outputId": "a67c81e7-9403-4f98-c541-f7f9160abf99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "X_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# use below code when running in local\n",
    "# import util_mnist_reader as mnist_reader\n",
    "# X_train, y_train = mnist_reader.load_mnist_for_CNN('data/fashion', kind='train')\n",
    "# X_test, y_test = mnist_reader.load_mnist_for_CNN('data/fashion', kind='t10k')\n",
    "\n",
    "# Load the fashion-mnist data from keras dataset | use this when using google collab\n",
    "import tensorflow as tf\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMqwJH6KfPqj"
   },
   "source": [
    "#### preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "J5Ote3tufPq0",
    "outputId": "cf75ed28-ff84-4771-fceb-f7b4d3105010",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) float32 (10000, 28, 28, 1) float32\n",
      "255.0 255.0\n",
      "1.0 1.0\n",
      "(48000, 28, 28, 1) (12000, 28, 28, 1) (48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "# reshaping so that it can be fed to auto-encoder\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "# type change to support in keras\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "print(X_train.shape, X_train.dtype, X_test.shape, X_test.dtype)\n",
    "\n",
    "# normalize the data\n",
    "print(np.max(X_train), np.max(X_test))\n",
    "X_train = X_train/np.max(X_train)\n",
    "X_test = X_test/np.max(X_test)\n",
    "print(np.max(X_train), np.max(X_test))\n",
    "\n",
    "# split data into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=13)\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lz2qacd0ymZQ"
   },
   "source": [
    "#### encoder, decoder and other functions common to Task 2 and Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcGI44U_fPrE"
   },
   "outputs": [],
   "source": [
    "# for task 3 too much ram requirement for conv4, so removing it for task 3\n",
    "def encoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    return conv3\n",
    "\n",
    "def decoder(conv3): \n",
    "    #decoder\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 64\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded\n",
    "\n",
    "def flatten(enco):\n",
    "    out = Flatten()(enco)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byTgfvTNymZd"
   },
   "source": [
    "#### defining our auto-encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZRWtjXI-fPrT",
    "outputId": "9247a6a5-8763-4aed-84e3-5f4c2386a630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 186,497\n",
      "Trainable params: 185,857\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# setting different parameters' values for the model\n",
    "batch_size = 480\n",
    "epochs = 200\n",
    "input_img = Input(shape = (28,28,1))\n",
    "num_classes = 10\n",
    "\n",
    "# define the auto-encoder model\n",
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n",
    "\n",
    "# print the summary/architecture of the autoencoder we are using\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIoFs3H0ymZw"
   },
   "source": [
    "#### train the auto-encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-o1go5JgfPrk",
    "outputId": "9cd7ac2b-caf2-4989-f448-d303229242c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.0292 - val_loss: 0.0301\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0126 - val_loss: 0.0150\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0089 - val_loss: 0.0077\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0052 - val_loss: 0.0046\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0049 - val_loss: 0.0097\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0025 - val_loss: 0.0047\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0015 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "autoencoder_train = autoencoder.fit(X_train, X_train, batch_size=480, epochs=epochs, verbose=1,validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5A3S4AOymZ8"
   },
   "source": [
    "#### Draw the training and validation loss vs epochs graph for training the auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "pIcumehGfPrs",
    "outputId": "cd17f9c8-6ca6-466b-9e14-7c68c39defde",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = autoencoder_train.history['loss']\n",
    "val_loss = autoencoder_train.history['val_loss']\n",
    "plt.figure()\n",
    "plt.plot(loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-8KxtyOymaH"
   },
   "source": [
    "#### Make only an encoder model using the same structure and weights as before with just an addition of flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "colab_type": "code",
    "id": "o08DD-pffPsU",
    "outputId": "3f6be2a3-9c2b-45d5-9c38-2b3c347f686d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "=================================================================\n",
      "Total params: 93,568\n",
      "Trainable params: 93,120\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encode = encoder(input_img)\n",
    "encoder_model = Model(input_img,flatten(encode))\n",
    "#encoder_model.layers\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "xxl3JIlgfPsr",
    "outputId": "8b952652-77da-496d-a773-1deb4ede6420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.09191477  0.23197442 -0.08947357 -0.1581791  -0.14265007\n",
      "   -0.19768147  0.06695522  0.08693453  0.05137896  0.14320579\n",
      "   -0.17349476 -0.07924609  0.3161732   0.14687526 -0.24022019\n",
      "   -0.18729201  0.05606147 -0.0321879  -0.09309539 -0.04845624\n",
      "   -0.11965645 -0.02582127  0.1289782   0.12898305 -0.03124579\n",
      "    0.12503724  0.09124305  0.14769474  0.02310151  0.2112457\n",
      "   -0.08924935 -0.29200545]]\n",
      "\n",
      " [[ 0.2848723   0.04996274 -0.11579301 -0.24367012  0.02214088\n",
      "   -0.3471586  -0.34291843  0.08075299  0.13557595 -0.26784047\n",
      "    0.26401252 -0.19768183  0.33575526 -0.1345207   0.14186315\n",
      "   -0.05032038  0.0657806   0.32597068  0.12444524  0.0118987\n",
      "    0.18314804 -0.00560663  0.3168614  -0.17797226 -0.19981682\n",
      "    0.15602612  0.07787874 -0.12149646  0.581587   -0.15753789\n",
      "    0.2997621  -0.2215946 ]]\n",
      "\n",
      " [[ 0.15653999  0.18192233  0.2468831  -0.01801955  0.1347526\n",
      "   -0.36348483 -0.21699096  0.19597593 -0.17635335  0.16301125\n",
      "   -0.18072025 -0.03765305 -0.19984914 -0.0954921   0.03843353\n",
      "    0.25079605  0.12287743 -0.20708095 -0.02263453  0.62749916\n",
      "   -0.05228222  0.07348015 -0.00604157  0.05230951 -0.26911318\n",
      "    0.12863922 -0.11734336 -0.27800718  0.29725134 -0.10909203\n",
      "    0.0240075  -0.07235464]]]\n",
      "[[[ 0.09191477  0.23197442 -0.08947357 -0.1581791  -0.14265007\n",
      "   -0.19768147  0.06695522  0.08693453  0.05137896  0.14320579\n",
      "   -0.17349476 -0.07924609  0.3161732   0.14687526 -0.24022019\n",
      "   -0.18729201  0.05606147 -0.0321879  -0.09309539 -0.04845624\n",
      "   -0.11965645 -0.02582127  0.1289782   0.12898305 -0.03124579\n",
      "    0.12503724  0.09124305  0.14769474  0.02310151  0.2112457\n",
      "   -0.08924935 -0.29200545]]\n",
      "\n",
      " [[ 0.2848723   0.04996274 -0.11579301 -0.24367012  0.02214088\n",
      "   -0.3471586  -0.34291843  0.08075299  0.13557595 -0.26784047\n",
      "    0.26401252 -0.19768183  0.33575526 -0.1345207   0.14186315\n",
      "   -0.05032038  0.0657806   0.32597068  0.12444524  0.0118987\n",
      "    0.18314804 -0.00560663  0.3168614  -0.17797226 -0.19981682\n",
      "    0.15602612  0.07787874 -0.12149646  0.581587   -0.15753789\n",
      "    0.2997621  -0.2215946 ]]\n",
      "\n",
      " [[ 0.15653999  0.18192233  0.2468831  -0.01801955  0.1347526\n",
      "   -0.36348483 -0.21699096  0.19597593 -0.17635335  0.16301125\n",
      "   -0.18072025 -0.03765305 -0.19984914 -0.0954921   0.03843353\n",
      "    0.25079605  0.12287743 -0.20708095 -0.02263453  0.62749916\n",
      "   -0.05228222  0.07348015 -0.00604157  0.05230951 -0.26911318\n",
      "    0.12863922 -0.11734336 -0.27800718  0.29725134 -0.10909203\n",
      "    0.0240075  -0.07235464]]]\n"
     ]
    }
   ],
   "source": [
    "# copy the weights from previously trained autoencoder model to our new model\n",
    "for l1,l2 in zip(encoder_model.layers[:8],autoencoder.layers[0:8]):\n",
    "    l1.set_weights(l2.get_weights())\n",
    "    \n",
    "# Run this to confirm weights are same in our new encoder model with flatten layer\n",
    "print(autoencoder.get_weights()[0][1])\n",
    "print(encoder_model.get_weights()[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0emoay61ymad"
   },
   "source": [
    "#### Use the encoder model to get the training input for the kmeans and Gaussian mixture models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcxFtiKWfPtE"
   },
   "outputs": [],
   "source": [
    "encoded_output = encoder_model.predict(X_train)\n",
    "#encoded_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIwXCKpayman"
   },
   "source": [
    "### Task 2: Build an Auto-Encoder based K-Means clustering model to cluster the condensed representation of the unlabeled fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzXJrEo8ymaq"
   },
   "source": [
    "#### Define the KMeans model and fit the model using the encoded_output found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aT1kiuV8fPtV",
    "outputId": "3f4fae2b-feb4-4cc9-8c70-863bceb75f61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=100,\n",
       "       n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, init='random', max_iter=100, tol=0.0001)\n",
    "kmeans.fit(encoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VSryqYAfPtn"
   },
   "source": [
    "#### Find the accuracy of the KMeans model for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "PoszZf94fPtt",
    "outputId": "c9ef85f8-9f80-43bf-8a75-f944c5f52512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[517  41  14 268 115   0 160   0   3   1]\n",
      " [  0   0   0   0   0  81   0  39   0 816]\n",
      " [153  23 298  64 206   6 261   0  25   5]\n",
      " [  5   0   3   1   4   0   2   0 403   1]\n",
      " [ 30 814   4 457  24   0  20   0   4   0]\n",
      " [  2   0   0   0   0 737   5 174  58  62]\n",
      " [  3   0   3   1   4   4  12   2 331   2]\n",
      " [  1   0   0   0   0 162   0 785  45 108]\n",
      " [252 115 197 201  95  10 278   0  76   3]\n",
      " [ 37   7 481   8 552   0 262   0  55   2]]\n",
      "Task 2: Accuracy on the Training dataset 52.44530623129291\n",
      "Task 2: Accuracy on the Testing dataset 51.87130739865332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_predict_train = kmeans.labels_\n",
    "y_predict_test = kmeans.predict(encoder_model.predict(X_test))\n",
    "\n",
    "# print(y_predict_train.shape, y_train.shape)\n",
    "score1 = 100 * completeness_score(y_train, y_predict_train)\n",
    "score2 = 100 * completeness_score(y_test, y_predict_test)\n",
    "conf_matrix = confusion_matrix(y_predict_test, y_test)\n",
    "print(conf_matrix)\n",
    "print(\"Task 2: Accuracy on the Training dataset\", score1)\n",
    "print(\"Task 2: Accuracy on the Testing dataset\", score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDo8JTsMymbU"
   },
   "source": [
    "### Task 3: Build an Auto-Encoder based Gaussian Mixture Model clustering model to cluster the condensed representation of the unlabeled fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpQp3O65wnKG"
   },
   "source": [
    "#### Define the Gaussian Mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "E52m-7DX3Kqv",
    "outputId": "e18ae2c2-ed77-4d51-9c63-48f6cfebe358"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
       "                means_init=None, n_components=10, n_init=1,\n",
       "                precisions_init=None, random_state=None, reg_covar=1e-06,\n",
       "                tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
       "                weights_init=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import mixture\n",
    "# 'spherical' 'tied' 'diag' 'full'\n",
    "cv_type = 'diag'\n",
    "gmm = mixture.GaussianMixture(n_components=10, covariance_type=cv_type)\n",
    "gmm.fit(encoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vCHG4wTymbw"
   },
   "source": [
    "#### Find the accuracy of the Gaussian Mixture model for Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "rDb3MHqH4xTR",
    "outputId": "0614a213-5b4f-4626-8b68-eb3943fae02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0 536   0 921   5  95]\n",
      " [ 90 107   9 621  67   0  64   0  13   0]\n",
      " [ 23   4  23   3  12   4  48   7 437   3]\n",
      " [  8   0   7  11  16   0   8   0 450   0]\n",
      " [  0   0   0   0   0 346   0  67   2 886]\n",
      " [ 60  37  41  58  37 114  58   5  72  15]\n",
      " [181   4 275  46 107   0 273   0   0   1]\n",
      " [602  18  17 171  92   0 190   0   3   0]\n",
      " [ 33   7 628  12 666   0 357   0  18   0]\n",
      " [  3 823   0  78   3   0   2   0   0   0]]\n",
      "Task 3: Accuracy on the Training dataset 58.26739037127447\n",
      "Task 3: Accuracy on the Testing dataset 56.258316333777444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_predict_train = gmm.predict(encoder_model.predict(X_train))\n",
    "y_predict_test = gmm.predict(encoder_model.predict(X_test))\n",
    "\n",
    "score1 = 100 * completeness_score(y_train, y_predict_train)\n",
    "score2 = 100 * completeness_score(y_test, y_predict_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_predict_test, y_test)\n",
    "print(conf_matrix)\n",
    "print(\"Task 3: Accuracy on the Training dataset\", score1)\n",
    "print(\"Task 3: Accuracy on the Testing dataset\", score2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main_final_changes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
